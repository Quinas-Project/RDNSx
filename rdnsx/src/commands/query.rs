//! Query command implementation

use std::sync::Arc;

use anyhow::Result;
use clap::Args;
use rdnsx_core::{DnsxClient, RecordType, ResponseCode, DnsRecord, CassandraExporter, CassandraConfig, ElasticsearchExporter, MongodbExporter, ResolverPool, WildcardFilter, Exporter, config::DnsxOptions, ConcurrentProcessor, ConcurrencyConfig, ProcessingMetrics, DomainStreamer, DnsCache, CachedDnsClient, AdaptiveBatchSizer};

use crate::cli::Config;
use crate::output_writer::OutputWriter;

#[derive(Args)]
pub struct QueryArgs {
    /// Domains to query
    #[arg(value_name = "DOMAIN")]
    pub domains: Vec<String>,

    /// Input file (default: stdin)
    #[arg(short, long)]
    pub list: Option<String>,

    /// DNS record types to query (can be repeated)
    #[arg(short = 't', long = "record-type", value_name = "TYPE", action = clap::ArgAction::Append)]
    pub record_type: Vec<String>,

    /// A records (default if no record types specified)
    #[arg(short, long)]
    pub a: bool,

    /// AAAA records
    #[arg(long)]
    pub aaaa: bool,

    /// CNAME records
    #[arg(long)]
    pub cname: bool,

    /// MX records
    #[arg(long)]
    pub mx: bool,

    /// TXT records
    #[arg(long)]
    pub txt: bool,

    /// NS records
    #[arg(long)]
    pub ns: bool,

    /// SOA records
    #[arg(long)]
    pub soa: bool,

    /// PTR records
    #[arg(long)]
    pub ptr: bool,

    /// SRV records
    #[arg(long)]
    pub srv: bool,

    /// CAA records
    #[arg(long)]
    pub caa: bool,

    /// CERT records
    #[arg(long)]
    pub cert: bool,

    /// DNAME records
    #[arg(long)]
    pub dname: bool,

    /// DNSKEY records
    #[arg(long)]
    pub dnskey: bool,

    /// DS records
    #[arg(long)]
    pub ds: bool,

    /// HINFO records
    #[arg(long)]
    pub hinfo: bool,

    /// HTTPS records
    #[arg(long)]
    pub https: bool,

    /// KEY records
    #[arg(long)]
    pub key: bool,

    /// LOC records
    #[arg(long)]
    pub loc: bool,

    /// NAPTR records
    #[arg(long)]
    pub naptr: bool,

    /// NSEC records
    #[arg(long)]
    pub nsec: bool,

    /// NSEC3 records
    #[arg(long)]
    pub nsec3: bool,

    /// OPT records
    #[arg(long)]
    pub opt: bool,

    /// RRSIG records
    #[arg(long)]
    pub rrsig: bool,

    /// SSHFP records
    #[arg(long)]
    pub sshfp: bool,

    /// SVCB records
    #[arg(long)]
    pub svcb: bool,

    /// TLSA records
    #[arg(long)]
    pub tlsa: bool,

    /// URI records
    #[arg(long)]
    pub uri: bool,

    /// ASN information
    #[arg(long)]
    pub asn: bool,

    /// Filter by response code (comma-separated)
    #[arg(long)]
    pub rcode: Option<String>,

    /// Domain for wildcard detection
    #[arg(short = 'w', long)]
    pub wildcard_domain: Option<String>,

    /// Response values only
    #[arg(long)]
    pub resp_only: bool,

    /// Use streaming mode for large files (reduces memory usage)
    #[arg(long)]
    pub stream: bool,

    /// Enable DNS response caching (reduces redundant queries)
    #[arg(long)]
    pub cache: bool,

    /// Cache TTL in seconds (default: 300)
    #[arg(long, default_value = "300")]
    pub cache_ttl: u64,

    /// Maximum cache size (default: 10000)
    #[arg(long, default_value = "10000")]
    pub cache_size: usize,

    /// Cassandra batch size (default: 1000)
    #[arg(long, default_value = "1000")]
    pub cassandra_batch_size: usize,

    /// Number of Cassandra worker threads (default: 4)
    #[arg(long, default_value = "4")]
    pub cassandra_workers: usize,
}

pub async fn run(args: QueryArgs, config: Config) -> Result<()> {
    // Determine record types to query
    let record_types = determine_record_types(&args);

    // Create DNS client
    let dns_options = DnsxOptions {
        resolvers: config.core_config.resolvers.servers.clone(),
        timeout: std::time::Duration::from_secs(config.core_config.resolvers.timeout),
        retries: config.core_config.resolvers.retries,
        concurrency: config.core_config.performance.threads,
        rate_limit: config.core_config.performance.rate_limit,
    };
    let client = DnsxClient::with_options(dns_options.clone())?;

    // Create wildcard filter if domain specified
    let wildcard_filter: Option<WildcardFilter> = if let Some(ref base_domain) = args.wildcard_domain {
        let resolver_pool = Arc::new(ResolverPool::new(&dns_options)?);
        Some(WildcardFilter::new(
            Some(base_domain.clone()),
            resolver_pool,
            10, // Default threshold: 10 domains pointing to same IP
        ))
    } else {
        None
    };

    // Parse response code filter
    let allowed_rcodes = parse_rcodes(&args.rcode)?;

    // Create output writer
    let mut output = OutputWriter::new(config.output_file.clone(), config.json_output, config.silent)?;

    // Create exporters if configured
    let mut es_exporter: Option<ElasticsearchExporter> = None;
    let mut mongo_exporter: Option<MongodbExporter> = None;
    let mut cassandra_exporter: Option<CassandraExporter> = None;

    if config.core_config.export.elasticsearch.enabled {
        es_exporter = Some(
            ElasticsearchExporter::new(
                &config.core_config.export.elasticsearch.url,
                &config.core_config.export.elasticsearch.index,
                config.core_config.export.batch_size,
            )
            .await
            .map_err(|e| anyhow::anyhow!("Failed to create Elasticsearch exporter: {}", e))?,
        );
    }

    if config.core_config.export.mongodb.enabled {
        mongo_exporter = Some(
            MongodbExporter::new(
                &config.core_config.export.mongodb.url,
                &config.core_config.export.mongodb.database,
                &config.core_config.export.mongodb.collection,
                config.core_config.export.batch_size,
            )
            .await
            .map_err(|e| anyhow::anyhow!("Failed to create MongoDB exporter: {}", e))?,
        );
    }

    if config.core_config.export.cassandra.enabled {
        let cassandra_config = CassandraConfig {
            contact_points: config.core_config.export.cassandra.contact_points.clone(),
            username: Some(config.core_config.export.cassandra.username.clone()),
            password: Some(config.core_config.export.cassandra.password.clone()),
            keyspace: config.core_config.export.cassandra.keyspace.clone(),
            table: config.core_config.export.cassandra.table.clone(),
            batch_size: args.cassandra_batch_size,
            num_workers: args.cassandra_workers,
            ..Default::default()
        };

        cassandra_exporter = Some(
            CassandraExporter::with_config(cassandra_config)
                .await
                .map_err(|e| anyhow::anyhow!("Failed to create Cassandra exporter: {}", e))?,
        );
    }

    // Determine if we should use streaming mode
    let use_streaming = args.stream || args.list.is_some(); // Auto-enable for files

    let domains: Vec<String> = if use_streaming && args.list.is_some() {
        // Use streaming for large files
        if !config.silent {
            eprintln!("Using streaming mode for large file processing");
        }
        Vec::new() // We'll stream domains directly
    } else {
        // Load all domains into memory for small lists or stdin
        let mut domains = read_domains(&args.list)?;
        domains.extend(args.domains.clone());

        if domains.is_empty() {
            if !config.silent {
                eprintln!("No domains to process. Use --list to specify a file or provide domains as arguments.");
            }
            return Ok(());
        }
        domains
    };

    // Create adaptive batch sizer if enabled
    let mut adaptive_batcher = AdaptiveBatchSizer::new(1000, 100, 10000);

    // Create concurrency configuration with adaptive batching
    let concurrency_config = ConcurrencyConfig {
        max_concurrent: config.core_config.performance.threads,
        batch_size: adaptive_batcher.current_size(),
        timeout: std::time::Duration::from_secs(config.core_config.resolvers.timeout),
        rate_limit: config.core_config.performance.rate_limit,
    };

    // Create cached client if caching is enabled
    let (client_clone, cached_client_ref): (Arc<dyn rdnsx_core::DnsQuery + Send + Sync>, Option<Arc<CachedDnsClient<DnsxClient>>>) = if args.cache {
        if !config.silent {
            eprintln!("DNS caching enabled (TTL: {}s, max size: {})", args.cache_ttl, args.cache_size);
        }
        let cache = DnsCache::new(args.cache_size, std::time::Duration::from_secs(args.cache_ttl));
        let cached_client = Arc::new(CachedDnsClient::new(client, cache));
        (cached_client.clone() as Arc<dyn rdnsx_core::DnsQuery + Send + Sync>, Some(cached_client))
    } else {
        (Arc::new(client) as Arc<dyn rdnsx_core::DnsQuery + Send + Sync>, None)
    };

    // Create the concurrent processor with all record types and domains
    let processor = ConcurrentProcessor::new(concurrency_config, {
        let record_types = record_types.clone();
        let client = Arc::clone(&client_clone);
        let allowed_rcodes = allowed_rcodes.clone();
        let wildcard_filter = wildcard_filter.clone();
        let silent = config.silent;

        move |domain: String| {
            let record_types = record_types.clone();
            let client = Arc::clone(&client);
            let allowed_rcodes = allowed_rcodes.clone();
            let wildcard_filter = wildcard_filter.clone();
            let silent = silent;

            Box::pin(async move {
                let mut all_records = Vec::new();

                // Query each record type for this domain
                for record_type in &record_types {
                    match client.query(&domain, *record_type).await {
                        Ok(mut records) => {
                            // Filter by response code if specified
                            if !allowed_rcodes.is_empty() {
                                records.retain(|r| allowed_rcodes.contains(&r.response_code));
                            }

                            // Apply wildcard filtering if enabled
                            let filtered_records = if let Some(ref filter) = wildcard_filter {
                                filter.filter(records.clone()).await
                                    .unwrap_or_else(|e| {
                                        if !silent {
                                            eprintln!("Warning: Wildcard filtering failed for {}: {}", domain, e);
                                        }
                                        records
                                    })
                            } else {
                                records
                            };

                            all_records.extend(filtered_records);
                        }
                        Err(e) => {
                            if !silent {
                                eprintln!("Error querying {} ({:?}): {}", domain, record_type, e);
                            }
                        }
                    }
                }

                Ok(all_records)
            })
        }
    });

    // Process domains concurrently with adaptive batching
    let (all_records, metrics) = if use_streaming && args.list.is_some() {
        // Streaming mode for large files with adaptive batching
        let file = std::fs::File::open(args.list.as_ref().unwrap())?;
        let reader = std::io::BufReader::new(file);
        let streamer = DomainStreamer::new(reader);

        let domain_iter = streamer.stream_domains().filter_map(|result| match result {
            Ok(domain) if !domain.is_empty() => Some(domain),
            Ok(_) => None, // Skip empty lines
            Err(e) => {
                eprintln!("Error reading domain: {}", e);
                None
            }
        });

        // Collect domains for adaptive batching
        let domains_vec: Vec<String> = domain_iter.collect();

        // Process with adaptive batching
        process_with_adaptive_batching(
            processor,
            domains_vec,
            &mut adaptive_batcher,
            !config.silent,
        ).await.map_err(anyhow::Error::from)?
    } else {
        // In-memory processing for smaller lists
        processor.process_stream(domains.into_iter()).await?
    };

    if !config.silent {
        eprintln!("Processed {} domains, collected {} records ({:.1} qps)",
                 metrics.total_domains, all_records.len(), metrics.queries_per_second);

        // Show cache statistics if caching was enabled
        if let Some(ref cached_client) = cached_client_ref {
            let cache_stats = cached_client.cache_stats();
            eprintln!("Cache: {} total entries ({} valid, {} expired)",
                     cache_stats.total_entries, cache_stats.valid_entries, cache_stats.expired_entries);
        }

        // Show Cassandra performance metrics if Cassandra export was enabled
        if config.core_config.export.cassandra.enabled {
            if let Some(ref cassandra) = cassandra_exporter {
                let metrics = cassandra.metrics();
                if metrics.total_records > 0 {
                    eprintln!("Cassandra: {} records inserted in {:.2}s ({:.1} rps), {} batches, {} errors, {} retries",
                             metrics.total_records,
                             metrics.total_insert_time.as_secs_f64(),
                             metrics.records_per_second,
                             metrics.batches_processed,
                             metrics.errors,
                             metrics.retries);
                }
            }
        }
    }

    // Output all records
    for record in all_records {
        output.write_record(&record, args.resp_only)?;

        // Export to Elasticsearch if configured
        if let Some(ref exporter) = es_exporter {
            if let Err(e) = exporter.export(record.clone()).await {
                if !config.silent {
                    eprintln!("Warning: Failed to export to Elasticsearch: {}", e);
                }
            }
        }

        // Export to MongoDB if configured
        if let Some(ref exporter) = mongo_exporter {
            if let Err(e) = exporter.export(record.clone()).await {
                if !config.silent {
                    eprintln!("Warning: Failed to export to MongoDB: {}", e);
                }
            }
        }

        // Export to Cassandra if configured
        if let Some(ref exporter) = cassandra_exporter {
            if let Err(e) = exporter.export(record.clone()).await {
                if !config.silent {
                    eprintln!("Warning: Failed to export to Cassandra: {}", e);
                }
            }
        }
    }

    // Flush exporters
    if let Some(ref exporter) = es_exporter {
        exporter.flush().await.map_err(|e| anyhow::anyhow!("Failed to flush Elasticsearch: {}", e))?;
    }
    if let Some(ref exporter) = mongo_exporter {
        exporter.flush().await.map_err(|e| anyhow::anyhow!("Failed to flush MongoDB: {}", e))?;
    }
    if let Some(ref exporter) = cassandra_exporter {
        exporter.flush().await.map_err(|e| anyhow::anyhow!("Failed to flush Cassandra: {}", e))?;
    }

    output.flush()?;
    Ok(())
}

fn determine_record_types(args: &QueryArgs) -> Vec<RecordType> {
    let mut types = Vec::new();

    // If --record-type is specified, use those
    if !args.record_type.is_empty() {
        for rt in &args.record_type {
            match rt.to_uppercase().as_str() {
                "A" => types.push(RecordType::A),
                "AAAA" => types.push(RecordType::Aaaa),
                "CNAME" => types.push(RecordType::Cname),
                "MX" => types.push(RecordType::Mx),
                "TXT" => types.push(RecordType::Txt),
                "NS" => types.push(RecordType::Ns),
                "SOA" => types.push(RecordType::Soa),
                "PTR" => types.push(RecordType::Ptr),
                "SRV" => types.push(RecordType::Srv),
                "CAA" => types.push(RecordType::Caa),
                "CERT" => types.push(RecordType::Cert),
                "DNAME" => types.push(RecordType::Dname),
                "DNSKEY" => types.push(RecordType::Dnskey),
                "DS" => types.push(RecordType::Ds),
                "HINFO" => types.push(RecordType::Hinfo),
                "HTTPS" => types.push(RecordType::Https),
                "KEY" => types.push(RecordType::Key),
                "LOC" => types.push(RecordType::Loc),
                "NAPTR" => types.push(RecordType::Naptr),
                "NSEC" => types.push(RecordType::Nsec),
                "NSEC3" => types.push(RecordType::Nsec3),
                "OPT" => types.push(RecordType::Opt),
                "RRSIG" => types.push(RecordType::Rrsig),
                "SSHFP" => types.push(RecordType::Sshfp),
                "SVCB" => types.push(RecordType::Svcb),
                "TLSA" => types.push(RecordType::Tlsa),
                "URI" => types.push(RecordType::Uri),
                _ => eprintln!("Warning: Unknown record type '{}', ignoring", rt),
            }
        }
        return types;
    }

    // Fall back to individual flags
    if args.aaaa {
        types.push(RecordType::Aaaa);
    }
    if args.cname {
        types.push(RecordType::Cname);
    }
    if args.mx {
        types.push(RecordType::Mx);
    }
    if args.txt {
        types.push(RecordType::Txt);
    }
    if args.ns {
        types.push(RecordType::Ns);
    }
    if args.soa {
        types.push(RecordType::Soa);
    }
    if args.ptr {
        types.push(RecordType::Ptr);
    }
    if args.srv {
        types.push(RecordType::Srv);
    }
    if args.caa {
        types.push(RecordType::Caa);
    }
    if args.cert {
        types.push(RecordType::Cert);
    }
    if args.dname {
        types.push(RecordType::Dname);
    }
    if args.dnskey {
        types.push(RecordType::Dnskey);
    }
    if args.ds {
        types.push(RecordType::Ds);
    }
    if args.hinfo {
        types.push(RecordType::Hinfo);
    }
    if args.https {
        types.push(RecordType::Https);
    }
    if args.key {
        types.push(RecordType::Key);
    }
    if args.loc {
        types.push(RecordType::Loc);
    }
    if args.naptr {
        types.push(RecordType::Naptr);
    }
    if args.nsec {
        types.push(RecordType::Nsec);
    }
    if args.nsec3 {
        types.push(RecordType::Nsec3);
    }
    if args.opt {
        types.push(RecordType::Opt);
    }
    if args.rrsig {
        types.push(RecordType::Rrsig);
    }
    if args.sshfp {
        types.push(RecordType::Sshfp);
    }
    if args.svcb {
        types.push(RecordType::Svcb);
    }
    if args.tlsa {
        types.push(RecordType::Tlsa);
    }
    if args.uri {
        types.push(RecordType::Uri);
    }

    // Default to A if no specific types requested
    if types.is_empty() || args.a {
        types.push(RecordType::A);
    }

    types
}

fn read_domains(input_file: &Option<String>) -> Result<Vec<String>> {
    let lines = if let Some(file) = input_file {
        std::fs::read_to_string(file)?.lines().map(|s| s.to_string()).collect()
    } else {
        // Read from stdin
        use std::io::{self, BufRead};
        let stdin = io::stdin();
        stdin.lock().lines().collect::<io::Result<Vec<String>>>()?
    };

    Ok(lines.into_iter().filter(|s| !s.trim().is_empty()).collect())
}

fn parse_rcodes(rcode_str: &Option<String>) -> Result<Vec<ResponseCode>> {
    if let Some(rcodes) = rcode_str {
        let mut result = Vec::new();
        for code_str in rcodes.split(',') {
            let code = match code_str.trim().to_uppercase().as_str() {
                "NOERROR" => ResponseCode::NoError,
                "SERVFAIL" => ResponseCode::ServFail,
                "NXDOMAIN" => ResponseCode::NxDomain,
                "REFUSED" => ResponseCode::Refused,
                "FORMERR" => ResponseCode::FormErr,
                "NOTIMP" => ResponseCode::NotImp,
                _ => continue, // Skip unknown codes
            };
            result.push(code);
        }
        Ok(result)
    } else {
        Ok(Vec::new()) // No filter
    }
}

/// Process domains with adaptive batch sizing based on performance
async fn process_with_adaptive_batching<F>(
    processor: ConcurrentProcessor<String, F>,
    domains: Vec<String>,
    adaptive_batcher: &mut AdaptiveBatchSizer,
    verbose: bool,
) -> rdnsx_core::error::Result<(Vec<DnsRecord>, ProcessingMetrics)>
where
    F: Fn(String) -> futures::future::BoxFuture<'static, rdnsx_core::error::Result<Vec<DnsRecord>>> + Send + Sync + 'static,
{
    let mut all_records = Vec::new();
    let mut total_metrics = ProcessingMetrics::default();
    let start_time = std::time::Instant::now();

    // Process in chunks with adaptive batch sizing
    let mut start_idx = 0;
    let mut iteration = 0;

    while start_idx < domains.len() {
        let batch_size = adaptive_batcher.current_size().min(domains.len() - start_idx);
        let end_idx = start_idx + batch_size;

        if verbose && iteration > 0 {
            eprintln!("Processing batch {}-{} (adaptive batch size: {})",
                     start_idx + 1, end_idx, batch_size);
        }

        // Create a new processor with the current batch size
        let batch_processor = ConcurrentProcessor::new(
            ConcurrencyConfig {
                max_concurrent: processor.config().max_concurrent,
                batch_size: batch_size.min(1000), // Cap internal batch size
                timeout: processor.config().timeout,
                rate_limit: processor.config().rate_limit,
            },
            {
                let query_fn = Arc::clone(processor.query_fn());
                move |domain: String| {
                    let query_fn = Arc::clone(&query_fn);
                    Box::pin(async move {
                        query_fn(domain).await
                    })
                }
            },
        );

        let batch_domains = domains[start_idx..end_idx].to_vec();
        let (batch_records, batch_metrics) = batch_processor.process_stream(batch_domains.into_iter()).await?;

        all_records.extend(batch_records);
        total_metrics.total_domains += batch_metrics.total_domains;
        total_metrics.successful_queries += batch_metrics.successful_queries;
        total_metrics.failed_queries += batch_metrics.failed_queries;
        total_metrics.total_query_time += batch_metrics.total_query_time;

        // Adjust batch size based on performance
        if batch_metrics.queries_per_second > 0.0 {
            adaptive_batcher.adjust(batch_metrics.queries_per_second);
        }

        start_idx = end_idx;
        iteration += 1;
    }

    // Calculate final metrics
    let total_time = start_time.elapsed();
    if total_metrics.total_domains > 0 {
        total_metrics.average_query_time = total_metrics.total_query_time / total_metrics.total_domains as u32;
    }
    if total_time.as_secs_f64() > 0.0 {
        total_metrics.queries_per_second = total_metrics.total_domains as f64 / total_time.as_secs_f64();
    }

    Ok((all_records, total_metrics))
}
